{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. MHA\n",
    "- source : https://github.com/huggingface/transformers/blob/98dda8ed03ac3f4af5733bdddaa1dab6a81e15c1/src/transformers/models/ctrl/modeling_ctrl.py#L88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        assert (\n",
    "            self.head_dim * num_heads == hidden_size\n",
    "        )\n",
    "        \n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def split_into_heads(self, x, batch_size):\n",
    "        x = x.reshape(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute([0, 2, 1, 3])\n",
    "    \n",
    "    def forward(self, values, keys, query, mask):\n",
    "        print(f\"==== 기본 정보 ====\")\n",
    "        print(f\"embedding size는 {self.hidden_size} 입니다.\")\n",
    "        print(f\"heads 수는 {self.num_heads} 입니다.\")\n",
    "        print(f\"head의 dimension은 {self.head_dim} 입니다.\")\n",
    "        print('\\n')\n",
    "        print(f\"==== 초기 정보 ====\")\n",
    "        print(f\"query.shape : {query.shape}\")\n",
    "        print(f\"keys.shape : {keys.shape}\")\n",
    "        print(f\"values.shape : {values.shape}\")\n",
    "        if mask != None:\n",
    "            print(f\"mask.shape : {mask.shape}\")\n",
    "\n",
    "        query = self.q_proj(query)\n",
    "        keys = self.k_proj(keys)\n",
    "        values = self.v_proj(values)\n",
    "        print('\\n')\n",
    "        print(\"==== 선형레이어를 통과합니다. ====\")\n",
    "        print(f\"query.shape : {query.shape}\")\n",
    "        print(f\"keys.shape : {keys.shape}\")\n",
    "        print(f\"values.shape : {values.shape}\")\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "        query = self.split_into_heads(query, batch_size)\n",
    "        keys = self.split_into_heads(keys, batch_size)\n",
    "        values = self.split_into_heads(values, batch_size)\n",
    "        print('\\n')\n",
    "        print(f\"==== Multi-Head에 의해 분할합니다 ====\")\n",
    "        print(f\"query.shape : {query.shape}\")\n",
    "        print(f\"keys.shape : {keys.shape}\")\n",
    "        print(f\"values.shape : {values.shape}\")\n",
    "        \n",
    "        \n",
    "        # Attention 수행 (최종 shape 형태 : [batch, head, query, key])\n",
    "        print('\\n')\n",
    "        print(\"==== Attention 연산을 수행합니다. ====\")\n",
    "        attention_score = torch.einsum(\"nqhd,nkhd->nhqk\", [query, keys])\n",
    "        print(f\"attention_score shape: {attention_score.shape}\")\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            \n",
    "        attention_distribution = torch.softmax(attention_score / (self.hidden_size ** (1/2)), dim=3)\n",
    "        print(f\"attention_distribution shape: {attention_distribution.shape}\")\n",
    "\n",
    "        attention_value_matrix = torch.einsum(\"nhql,nlhd->nqhd\", [attention_distribution, values]).reshape(\n",
    "            batch_size, -1, self.num_heads * self.head_dim\n",
    "        )\n",
    "        print(f\"attention_value_matrix shape : {attention_value_matrix.shape}\")\n",
    "        \n",
    "        out = self.o_proj(attention_value_matrix)\n",
    "        print(f\"out shape : {out.shape}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA 레이어 객체 지정\n",
    "hidden_size = 512\n",
    "num_heads = 8\n",
    "attention = MultiHeadAttention(hidden_size, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 기본 정보 ====\n",
      "embedding size는 512 입니다.\n",
      "heads 수는 8 입니다.\n",
      "head의 dimension은 64 입니다.\n",
      "\n",
      "\n",
      "==== 초기 정보 ====\n",
      "query.shape : torch.Size([1, 1, 512])\n",
      "keys.shape : torch.Size([1, 1, 512])\n",
      "values.shape : torch.Size([1, 1, 512])\n",
      "\n",
      "\n",
      "==== 선형레이어를 통과합니다. ====\n",
      "query.shape : torch.Size([1, 1, 512])\n",
      "keys.shape : torch.Size([1, 1, 512])\n",
      "values.shape : torch.Size([1, 1, 512])\n",
      "\n",
      "\n",
      "==== Multi-Head에 의해 분할합니다 ====\n",
      "query.shape : torch.Size([1, 8, 1, 64])\n",
      "keys.shape : torch.Size([1, 8, 1, 64])\n",
      "values.shape : torch.Size([1, 8, 1, 64])\n",
      "\n",
      "\n",
      "==== Attention 연산을 수행합니다. ====\n",
      "attention_score shape: torch.Size([1, 1, 8, 8])\n",
      "attention_distribution shape: torch.Size([1, 1, 8, 8])\n",
      "attention_value_matrix shape : torch.Size([1, 1, 512])\n",
      "out shape : torch.Size([1, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "# 더미 입력 데이터 생성\n",
    "N, seq_length = 1, 1  # 배치 크기와 시퀀스 길이\n",
    "dummy = torch.rand((N, seq_length, hidden_size))\n",
    "dummy_mask = None  # 필요한 경우 마스크를 사용할 수 있습니다.\n",
    "\n",
    "# 멀티 헤드 주의 레이어 실행\n",
    "output = attention(dummy, dummy, dummy, dummy_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MQA, GQA \n",
    "- source : https://github.com/huggingface/transformers/blob/98dda8ed03ac3f4af5733bdddaa1dab6a81e15c1/src/transformers/models/llama/modeling_llama.py#L317 \n",
    "- LlamaAttention 클래스 부분확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_key_value_heads가 1이면, MQA\n",
    "# num_key_value_heads가 2이상이면, GQA\n",
    "\n",
    "import math\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size, \n",
    "                 num_heads,\n",
    "                 num_key_value_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size # 512\n",
    "        self.num_heads = num_heads # 8\n",
    "        self.head_dim = self.hidden_size // self.num_heads # 64\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        \n",
    "    def repeat_kv(self, \n",
    "                  hidden_states: torch.Tensor, \n",
    "                  n_rep: int) -> torch.Tensor:\n",
    "        \n",
    "        batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "        if n_rep == 1:\n",
    "            return hidden_states\n",
    "        hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "        return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "    def forward(self,values,keys,query):\n",
    "        print(f\"==== 기본 정보 ====\")\n",
    "        print(f\"embedding size는 {self.hidden_size} 입니다.\")\n",
    "        print(f\"heads 수는 {self.num_heads} 입니다.\")\n",
    "        print(f\"head의 dimension은 {self.head_dim} 입니다.\")\n",
    "        print(f\"head의 dimension은 {self.head_dim} 입니다.\")\n",
    "        print('\\n')\n",
    "        print(f\"==== 초기 정보 ====\")\n",
    "        print(f\"query.shape : {query.shape}\")\n",
    "        print(f\"keys.shape : {keys.shape}\")\n",
    "        print(f\"values.shape : {values.shape}\")\n",
    "        \n",
    "        bsz, q_len, _ = query.size()\n",
    "        query_states = self.q_proj(query)\n",
    "        key_states = self.k_proj(keys)\n",
    "        value_states = self.v_proj(values)\n",
    "        print('\\n')\n",
    "        print(\"==== 선형레이어를 통과합니다. ====\")\n",
    "        print(f\"query.shape : {query_states.shape}\")\n",
    "        print(f\"keys.shape : {key_states.shape}\")\n",
    "        print(f\"values.shape : {value_states.shape}\")\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = self.repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = self.repeat_kv(value_states, self.num_key_value_groups)\n",
    "        print('\\n')\n",
    "        print(f\"==== Query는 Multi-Head에 의해 분할하며, Key와 Value는 동일한 값들로 복제합니다. ====\")\n",
    "        print(f\"query.shape : {query_states.shape}\")\n",
    "        print(f\"keys.shape : {key_states.shape}\")\n",
    "        print(f\"values.shape : {value_states.shape}\")\n",
    "        attention_score = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        print(f\"attention_score shape: {attention_score.shape}\")\n",
    "        attention_distribution = nn.functional.softmax(attention_score, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        print(f\"attention_distribution shape: {attention_distribution.shape}\")\n",
    "        attention_value_matrix = torch.matmul(attention_distribution, value_states)\n",
    "        print(f\"attention_value_matrix shape: {attention_value_matrix.shape}\")\n",
    "        attention_value_matrix = attention_value_matrix.transpose(1, 2).contiguous()\n",
    "        print(f\"attention_value_matrix shape: {attention_value_matrix.shape}\")\n",
    "        attention_value_matrix = attention_value_matrix.reshape(bsz, q_len, self.hidden_size)\n",
    "        print(f\"attention_value_matrix shape: {attention_value_matrix.shape}\")\n",
    "        attn_output = self.o_proj(attention_value_matrix)\n",
    "        print(f\"out shape : {attn_output.shape}\")\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 기본 정보 ====\n",
      "embedding size는 512 입니다.\n",
      "heads 수는 8 입니다.\n",
      "head의 dimension은 64 입니다.\n",
      "head의 dimension은 64 입니다.\n",
      "\n",
      "\n",
      "==== 초기 정보 ====\n",
      "query.shape : torch.Size([1, 1, 512])\n",
      "keys.shape : torch.Size([1, 1, 512])\n",
      "values.shape : torch.Size([1, 1, 512])\n",
      "\n",
      "\n",
      "==== 선형레이어를 통과합니다. ====\n",
      "query.shape : torch.Size([1, 1, 512])\n",
      "keys.shape : torch.Size([1, 1, 128])\n",
      "values.shape : torch.Size([1, 1, 128])\n",
      "\n",
      "\n",
      "==== Query는 Multi-Head에 의해 분할하며, Key와 Value는 동일한 값들로 복제합니다. ====\n",
      "query.shape : torch.Size([1, 8, 1, 64])\n",
      "keys.shape : torch.Size([1, 8, 1, 64])\n",
      "values.shape : torch.Size([1, 8, 1, 64])\n",
      "attention_score shape: torch.Size([1, 8, 1, 1])\n",
      "attention_distribution shape: torch.Size([1, 8, 1, 1])\n",
      "attention_value_matrix shape: torch.Size([1, 8, 1, 64])\n",
      "attention_value_matrix shape: torch.Size([1, 1, 8, 64])\n",
      "attention_value_matrix shape: torch.Size([1, 1, 512])\n",
      "out shape : torch.Size([1, 1, 512])\n",
      "torch.Size([1, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "hidden_size = 512\n",
    "num_heads = 8\n",
    "num_key_value_heads = 2 \n",
    "model = MultiQueryAttention(hidden_size, \n",
    "                             num_heads,\n",
    "                             num_key_value_heads)\n",
    "\n",
    "# 더미 입력 데이터 생성\n",
    "N = 1  # 배치 크기\n",
    "seq_length = 1  # 시퀀스 길이\n",
    "\n",
    "dummy = torch.rand((N, seq_length, hidden_size))\n",
    "dummy_mask = None  # 필요한 경우 마스크를 사용할 수 있습니다.\n",
    "\n",
    "# 모델에 데이터 전달\n",
    "attn_output = model(dummy, dummy, dummy)\n",
    "print(attn_output.shape)  # 출력의 차원 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
